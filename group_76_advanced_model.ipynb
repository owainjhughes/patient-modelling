{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    55.0\n",
      "1    40.0\n",
      "2    37.0\n",
      "3    25.0\n",
      "4    24.0\n",
      "Name: AGE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "# Improved Data Cleansing\n",
    "def removeUnnecessaryColumns(df_patients):\n",
    "    df_patients.drop(columns=['index', 'PATIENT_TYPE'], inplace=True)\n",
    "\n",
    "# Identifying and handling missing values\n",
    "def handleMissingValues(df_patients):\n",
    "\n",
    "    # Make ? in pregnant 1 instead of deleting as ? means male\n",
    "    df_patients['PREGNANT'] = df_patients['PREGNANT'].replace('?', 1)\n",
    "\n",
    "    #Make all 9999-99-99 dates NaT then an average date \n",
    "    df_patients['DATE_DIED'] = pd.to_datetime(df_patients['DATE_DIED'], errors='coerce')\n",
    "    average_date = pd.to_datetime(df_patients['DATE_DIED'].dropna().astype(int).mean(), unit='ns')\n",
    "    df_patients['DATE_DIED'] = df_patients['DATE_DIED'].fillna(average_date)\n",
    "    \n",
    "    for col in df_patients.select_dtypes(include=['object']).columns:\n",
    "        df_patients[col] = df_patients[col].replace('?', np.nan)\n",
    "        df_patients[col] = LabelEncoder().fit_transform(df_patients[col].astype(str))\n",
    "\n",
    "    # Apply KNNImputer to handle missing values\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    numeric_cols = df_patients.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df_patients[numeric_cols] = imputer.fit_transform(df_patients[numeric_cols])\n",
    "\n",
    "# Outlier Detection and Removal using IQR\n",
    "def outlierDetection(df_patients):\n",
    "    numeric_cols = df_patients.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "    Q1 = df_patients[numeric_cols].quantile(0.25)\n",
    "    Q3 = df_patients[numeric_cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    df_patients = df_patients[~((df_patients[numeric_cols] < (Q1 - 1.5 * IQR)) | \n",
    "                                (df_patients[numeric_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    \n",
    "\n",
    "#Normalization of the AGE column using StandardScaler\n",
    "def normalizeData(df_patients):\n",
    "    print(df_patients['AGE'].head())\n",
    "    scaler = StandardScaler()\n",
    "    df_patients['AGE'] = scaler.fit_transform(df_patients[['AGE']])\n",
    "\n",
    "# Loading the dataset from the CSV file\n",
    "data_file = 'Dataset.csv'\n",
    "df_patients = pd.read_csv(data_file, low_memory=False)\n",
    "\n",
    "# Clean the dataset\n",
    "removeUnnecessaryColumns(df_patients)\n",
    "handleMissingValues(df_patients)\n",
    "outlierDetection(df_patients)\n",
    "normalizeData(df_patients)\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "df_patients.to_csv('deep_cleaned_patients.csv', index=False)\n",
    "\n",
    "# Checking the data types\n",
    "df_patients['SEX'] = df_patients['SEX'].astype('object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5573513-844b-4ef9-ba1c-5b5411841b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.198.133.54:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BigDataAnalysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x263c3b6da90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting Spark properly with a custom temp directory to avoid shuffle errors\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Set a safe local temp directory\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"C:/temp/spark\"\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"BigDataAnalysis\").getOrCreate()\n",
    "spark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45362f19-0260-47a9-9006-1c7ed76ee3c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+--------------------+-------+---------+-------------------+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|USMER|MEDICAL_UNIT|SEX|           DATE_DIED|INTUBED|PNEUMONIA|                AGE|PREGNANT|DIABETES|COPD|ASTHMA|INMSUPR|HIPERTENSION|OTHER_DISEASE|CARDIOVASCULAR|OBESITY|RENAL_CHRONIC|TOBACCO|CLASIFFICATION_FINAL|ICU|\n",
      "+-----+------------+---+--------------------+-------+---------+-------------------+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "|  2.0|         1.0|2.0| 2020-09-06 00:00:00|    0.0|      1.0|0.11245548466355285|     0.0|     0.0| 1.0|   1.0|    1.0|         1.0|          1.0|           1.0|    1.0|          1.0|    1.0|                 3.0|1.0|\n",
      "|  2.0|         1.0|1.0|2020-06-25 08:06:...|    1.0|      0.0|-0.6439819373845154|     1.0|     1.0| 1.0|   1.0|    1.0|         1.0|          1.0|           1.0|    1.0|          1.0|    1.0|                 3.0|1.0|\n",
      "|  2.0|         1.0|1.0|2020-06-25 08:06:...|    1.0|      1.0|-0.7952694217941291|     1.0|     0.0| 1.0|   1.0|    1.0|         0.0|          1.0|           1.0|    0.0|          1.0|    1.0|                 3.0|1.0|\n",
      "|  2.0|         1.0|1.0|2020-06-25 08:06:...|    1.0|      1.0|-1.4004193594325836|     1.0|     1.0| 1.0|   1.0|    1.0|         1.0|          1.0|           1.0|    1.0|          1.0|    1.0|                 3.0|1.0|\n",
      "|  2.0|         1.0|2.0|2020-06-25 08:06:...|    1.0|      1.0|-1.4508485209024549|     0.0|     1.0| 1.0|   1.0|    1.0|         1.0|          1.0|           1.0|    1.0|          1.0|    1.0|                 3.0|1.0|\n",
      "+-----+------------+---+--------------------+-------+---------+-------------------+--------+--------+----+------+-------+------------+-------------+--------------+-------+-------------+-------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- USMER: double (nullable = true)\n",
      " |-- MEDICAL_UNIT: double (nullable = true)\n",
      " |-- SEX: double (nullable = true)\n",
      " |-- DATE_DIED: timestamp (nullable = true)\n",
      " |-- INTUBED: double (nullable = true)\n",
      " |-- PNEUMONIA: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- PREGNANT: double (nullable = true)\n",
      " |-- DIABETES: double (nullable = true)\n",
      " |-- COPD: double (nullable = true)\n",
      " |-- ASTHMA: double (nullable = true)\n",
      " |-- INMSUPR: double (nullable = true)\n",
      " |-- HIPERTENSION: double (nullable = true)\n",
      " |-- OTHER_DISEASE: double (nullable = true)\n",
      " |-- CARDIOVASCULAR: double (nullable = true)\n",
      " |-- OBESITY: double (nullable = true)\n",
      " |-- RENAL_CHRONIC: double (nullable = true)\n",
      " |-- TOBACCO: double (nullable = true)\n",
      " |-- CLASIFFICATION_FINAL: double (nullable = true)\n",
      " |-- ICU: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading in the cleaned patient dataset\n",
    "df = spark.read.csv(\"deep_cleaned_patients.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Quick look at first few rows\n",
    "df.show(5)\n",
    "\n",
    "# Checking what the structure of the data looks like\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49fd6447-7d57-4cef-9370-7195a7137860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+-------+------------+--------+-------------------------------------+\n",
      "|AGE                |OBESITY|TOBACCO|HIPERTENSION|DIABETES|features                             |\n",
      "+-------------------+-------+-------+------------+--------+-------------------------------------+\n",
      "|0.11245548466355285|1.0    |1.0    |1.0         |0.0     |[0.11245548466355285,1.0,1.0,1.0,0.0]|\n",
      "|-0.6439819373845154|1.0    |1.0    |1.0         |1.0     |[-0.6439819373845154,1.0,1.0,1.0,1.0]|\n",
      "|-0.7952694217941291|0.0    |1.0    |0.0         |0.0     |(5,[0,2],[-0.7952694217941291,1.0])  |\n",
      "|-1.4004193594325836|1.0    |1.0    |1.0         |1.0     |[-1.4004193594325836,1.0,1.0,1.0,1.0]|\n",
      "|-1.4508485209024549|1.0    |1.0    |1.0         |1.0     |[-1.4508485209024549,1.0,1.0,1.0,1.0]|\n",
      "+-------------------+-------+-------+------------+--------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Picking out the main features to use in the model\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = [\"AGE\", \"OBESITY\", \"TOBACCO\", \"HIPERTENSION\", \"DIABETES\"]\n",
    "\n",
    "# Combining them into one feature column\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Just checking that the features column looks alright\n",
    "df_transformed.select(\"AGE\", \"OBESITY\", \"TOBACCO\", \"HIPERTENSION\", \"DIABETES\", \"features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c35c19-d288-4c88-ba3b-7f9227120bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[0.11245548466355...|         0|\n",
      "|[-0.6439819373845...|         2|\n",
      "|(5,[0,2],[-0.7952...|         0|\n",
      "|[-1.4004193594325...|         2|\n",
      "|[-1.4508485209024...|         2|\n",
      "|[-1.1482735520832...|         2|\n",
      "|[1.37318452141033...|         1|\n",
      "|[0.31417213054303...|         0|\n",
      "|[-0.3918361300351...|         2|\n",
      "|[-0.3918361300351...|         2|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using KMeans to group patients based on the features\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=3, seed=42, featuresCol=\"features\")\n",
    "kmeans_model = kmeans.fit(df_transformed)\n",
    "clusters = kmeans_model.transform(df_transformed)\n",
    "\n",
    "# Checking what cluster each patient ended up in\n",
    "clusters.select(\"features\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32bc6fef-fc80-4a88-83da-a42a1346e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+-----+\n",
      "|prediction|ICU|count|\n",
      "+----------+---+-----+\n",
      "|         2|1.0|57828|\n",
      "|         1|0.0| 6614|\n",
      "|         0|2.0| 1532|\n",
      "|         0|0.0| 4653|\n",
      "|         1|2.0| 2230|\n",
      "|         2|2.0| 3726|\n",
      "|         1|1.0|70650|\n",
      "|         2|0.0| 5591|\n",
      "|         0|1.0|47207|\n",
      "+----------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Seeing how ICU cases are spread across each cluster\n",
    "clusters.groupBy(\"prediction\", \"ICU\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8341f32-9e71-4c7c-b9ee-0dd291df7025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+---+----------+-------------------------------------------------------------+\n",
      "|features                             |ICU|prediction|probability                                                  |\n",
      "+-------------------------------------+---+----------+-------------------------------------------------------------+\n",
      "|[0.11245548466355285,1.0,1.0,1.0,0.0]|1.0|1.0       |[0.07819016120327144,0.890493321909399,0.031316516887329594] |\n",
      "|[-0.6439819373845154,1.0,1.0,1.0,1.0]|1.0|1.0       |[0.07786334886867124,0.8905219366081505,0.03161471452317827] |\n",
      "|(5,[0,2],[-0.7952694217941291,1.0])  |1.0|1.0       |[0.09669544402339211,0.871601532575399,0.03170302340120894]  |\n",
      "|[-1.4004193594325836,1.0,1.0,1.0,1.0]|1.0|1.0       |[0.07607678129615723,0.886954532440889,0.03696868626295389]  |\n",
      "|[-1.4508485209024549,1.0,1.0,1.0,1.0]|1.0|1.0       |[0.07607678129615723,0.886954532440889,0.03696868626295389]  |\n",
      "|[-1.1482735520832275,1.0,1.0,1.0,1.0]|1.0|1.0       |[0.0761614775417014,0.890143676796199,0.03369484566209967]   |\n",
      "|[1.3731845214103333,1.0,1.0,0.0,1.0] |0.0|1.0       |[0.07804586970579672,0.8911539653983166,0.030800164895886707]|\n",
      "|[0.3141721305430377,1.0,0.0,1.0,0.0] |0.0|1.0       |[0.07822444703122125,0.8903630587485961,0.031412494220182594]|\n",
      "|[-0.3918361300351593,1.0,1.0,1.0,1.0]|1.0|1.0       |[0.07786334886867124,0.8905219366081505,0.03161471452317827] |\n",
      "|[-0.3918361300351593,1.0,1.0,1.0,1.0]|0.0|1.0       |[0.07786334886867124,0.8905219366081505,0.03161471452317827] |\n",
      "+-------------------------------------+---+----------+-------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training a Random Forest model to predict ICU\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"ICU\", featuresCol=\"features\", numTrees=100)\n",
    "rf_model = rf.fit(df_transformed)\n",
    "\n",
    "# Making predictions on full dataset\n",
    "rf_predictions = rf_model.transform(df_transformed)\n",
    "rf_predictions.select(\"features\", \"ICU\", \"prediction\", \"probability\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a4d74f4-05b2-42bc-8462-f8c6c4361beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|ICU| count|\n",
      "+---+------+\n",
      "|2.0|  7488|\n",
      "|1.0|351566|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Balancing the dataset by oversampling ICU = 1 cases\n",
    "\n",
    "# Splitting the data into ICU = 1 (minority) and ICU = 2 (majority)\n",
    "minority = df_transformed.filter(df_transformed[\"ICU\"] == 1)\n",
    "majority = df_transformed.filter(df_transformed[\"ICU\"] == 2)\n",
    "\n",
    "# Duplicating the minority class to balance it\n",
    "oversampled = minority.sample(withReplacement=True, fraction=2.0)\n",
    "\n",
    "# Putting both groups back together\n",
    "balanced_data = majority.union(oversampled)\n",
    "\n",
    "# Shuffling the data\n",
    "from pyspark.sql.functions import rand\n",
    "balanced_data = balanced_data.orderBy(rand())\n",
    "\n",
    "# Checking how balanced it is now\n",
    "balanced_data.groupBy(\"ICU\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df6d577-e671-4ec4-9d31-a5226446b7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+---+----------+---------------------------------------------+\n",
      "|features                              |ICU|prediction|probability                                  |\n",
      "+--------------------------------------+---+----------+---------------------------------------------+\n",
      "|[-0.19011948415567445,0.0,1.0,0.0,1.0]|1.0|1.0       |[0.0,0.979783873151759,0.020216126848241072] |\n",
      "|[0.3646012920129089,1.0,1.0,1.0,1.0]  |1.0|1.0       |[0.0,0.9795425539343663,0.02045744606563369] |\n",
      "|[0.3141721305430377,1.0,1.0,0.0,1.0]  |1.0|1.0       |[0.0,0.979783873151759,0.020216126848241072] |\n",
      "|[0.8184637452417498,1.0,1.0,1.0,1.0]  |1.0|1.0       |[0.0,0.9795425539343663,0.02045744606563369] |\n",
      "|[-0.0388319997460608,1.0,1.0,1.0,0.0] |1.0|1.0       |[0.0,0.9798422311243377,0.020157768875662296]|\n",
      "|[0.8688929067116211,1.0,1.0,0.0,0.0]  |1.0|1.0       |[0.0,0.9800835503417303,0.019916449658269678]|\n",
      "|[-0.8456985832640003,1.0,1.0,1.0,1.0] |1.0|1.0       |[0.0,0.9795425539343663,0.02045744606563369] |\n",
      "|[1.5244720058199468,1.0,1.0,1.0,1.0]  |1.0|1.0       |[0.0,0.9795425539343663,0.02045744606563369] |\n",
      "|[-0.4926944529749017,1.0,1.0,0.0,0.0] |1.0|1.0       |[0.0,0.9800835503417303,0.019916449658269678]|\n",
      "|[0.616747099362265,1.0,1.0,1.0,0.0]   |1.0|1.0       |[0.0,0.9798422311243377,0.020157768875662296]|\n",
      "+--------------------------------------+---+----------+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model again on the balanced dataset\n",
    "rf_balanced = RandomForestClassifier(labelCol=\"ICU\", featuresCol=\"features\", numTrees=100)\n",
    "rf_balanced_model = rf_balanced.fit(balanced_data)\n",
    "\n",
    "# Making new predictions\n",
    "balanced_predictions = rf_balanced_model.transform(balanced_data)\n",
    "balanced_predictions.select(\"features\", \"ICU\", \"prediction\", \"probability\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753510b9-1f7e-46c4-90e2-14224694747a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Checking how accurate the model is\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ICU\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(balanced_predictions)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce812bad-aa13-4270-9dc0-7af63f4faf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.97\n",
      "Precision: 0.96\n",
      "Recall: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Extra evaluation metrics\n",
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"ICU\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = f1_eval.evaluate(balanced_predictions)\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "precision_eval = MulticlassClassificationEvaluator(labelCol=\"ICU\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = precision_eval.evaluate(balanced_predictions)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "recall_eval = MulticlassClassificationEvaluator(labelCol=\"ICU\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = recall_eval.evaluate(balanced_predictions)\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "931f8be8-391a-4429-a725-cfe95a51c86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train count: 287628\n",
      "Test count: 71421\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into train and test sets\n",
    "train_data, test_data = balanced_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Just checking how many rows in each\n",
    "print(\"Train count:\", train_data.count())\n",
    "print(\"Test count:\", test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21730c3a-da6c-4a97-9dbc-c39295095775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+---+----------+---------------------------------------------+\n",
      "|features                             |ICU|prediction|probability                                  |\n",
      "+-------------------------------------+---+----------+---------------------------------------------+\n",
      "|[-2.6107192347094927,1.0,1.0,1.0,1.0]|2.0|1.0       |[0.0,0.9723290744811335,0.027670925518866508]|\n",
      "|[-2.661148396179364,1.0,1.0,1.0,1.0] |2.0|1.0       |[0.0,0.9723290744811335,0.027670925518866508]|\n",
      "|[0.616747099362265,1.0,1.0,1.0,1.0]  |1.0|1.0       |[0.0,0.979361343802665,0.020638656197334876] |\n",
      "|[-0.7448402603242578,1.0,1.0,0.0,0.0]|1.0|1.0       |[0.0,0.980289444739156,0.01971055526084399]  |\n",
      "|[0.11245548466355285,1.0,1.0,0.0,0.0]|1.0|1.0       |[0.0,0.980289444739156,0.01971055526084399]  |\n",
      "|[1.6757594902295605,1.0,1.0,0.0,0.0] |1.0|1.0       |[0.0,0.9794794843676553,0.02052051563234471] |\n",
      "|[-0.8961277447338715,1.0,1.0,1.0,1.0]|1.0|1.0       |[0.0,0.979361343802665,0.020638656197334876] |\n",
      "|[-0.6439819373845154,1.0,0.0,1.0,0.0]|1.0|1.0       |[0.0,0.9798315176378312,0.020168482362168894]|\n",
      "|[-0.3918361300351593,1.0,1.0,1.0,0.0]|1.0|1.0       |[0.0,0.9800844412802718,0.019915558719728273]|\n",
      "|[-0.3918361300351593,1.0,1.0,1.0,1.0]|1.0|1.0       |[0.0,0.979361343802665,0.020638656197334876] |\n",
      "+-------------------------------------+---+----------+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training model on just the training data\n",
    "final_rf = RandomForestClassifier(labelCol=\"ICU\", featuresCol=\"features\", numTrees=100)\n",
    "final_model = final_rf.fit(train_data)\n",
    "\n",
    "# Predicting on test data\n",
    "test_preds = final_model.transform(test_data)\n",
    "test_preds.select(\"features\", \"ICU\", \"prediction\", \"probability\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68087c66-d360-4d14-8b0a-95710a83cc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "F1 Score: 0.97\n",
      "Precision: 0.96\n",
      "Recall: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on test data\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ICU\", predictionCol=\"prediction\")\n",
    "\n",
    "acc = evaluator.setMetricName(\"accuracy\").evaluate(test_preds)\n",
    "f1 = evaluator.setMetricName(\"f1\").evaluate(test_preds)\n",
    "prec = evaluator.setMetricName(\"weightedPrecision\").evaluate(test_preds)\n",
    "rec = evaluator.setMetricName(\"weightedRecall\").evaluate(test_preds)\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Precision: {prec:.2f}\")\n",
    "print(f\"Recall: {rec:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
